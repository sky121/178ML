{
 "cells": [
  {
   "source": [
    "from scholars import *\n",
    "X = np.genfromtxt('data/X_train.txt', delimiter=',')\n",
    "Y = np.genfromtxt('data/Y_train.txt', delimiter=',')\n",
    "X_test = np.genfromtxt('data/X_test.txt', delimiter=',')\n",
    "X, Y = ml.shuffleData(X, Y)\n",
    "Xtr,Xva,Ytr,Yva = ml.splitData(X,Y,0.75) \n",
    "np.random.seed(0)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "No module named xgboost",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a592c1cc71c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscholars\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/X_train.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/Y_train.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/X_test.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffleData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/skyh/Desktop/Current Classes/CS178/FinalProject/scholars.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmltools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mRandomForest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named xgboost"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HELPER FUNCTIONS\n",
    "\n",
    "def stackPredictions(predictions): #TAKES MEAN OF PREDICTIONS AND RETURNS THE FINAL PREDICTION MEAN\n",
    "    num_data = len(predictions[0])\n",
    "    finalPrediction = [0]*num_data\n",
    "    for j in range(num_data):\n",
    "        finalPrediction[j] = np.mean(np.array(predictions)[:, j])\n",
    "    return finalPrediction\n",
    "\n",
    "\n",
    "def convertToFinalPredictions(pred): #CONVERTS ALL PREDICITONS TO 0 AND 1\n",
    "    final_pred = []\n",
    "    for i in pred:\n",
    "        if i>0.5:\n",
    "            final_pred.append(1)\n",
    "        else:\n",
    "            final_pred.append(0)\n",
    "    return final_pred\n",
    "\n",
    "\n",
    "def PrintValidationErrorofPredictions(preds): #PRINTS THE ERROR OF EACH PREDICTION ON THE VALIDATION DATA\n",
    "    i = 0\n",
    "    for p in preds:\n",
    "        fp = convertToFinalPredictions(p)\n",
    "        print(i,\"Prediction ERROR: \", 1-sum(fp==Yva)/float(len(Yva)))\n",
    "        i+=1\n",
    "\n",
    "def calculateWeight(predictions): #RETURNS THE WEIGHTS FOR EACH PREDICITON\n",
    "    numOfleaners = len(predictions)\n",
    "    predictions = np.array([np.array(convertToFinalPredictions(pred)) for pred in predictions])\n",
    "    weight = np.zeros(numOfleaners)\n",
    "    for indx, trueClass in enumerate(Yva):\n",
    "        for i in range(numOfleaners):\n",
    "            if predictions[i,indx] == trueClass:\n",
    "                weight[i] += 1\n",
    "            else:\n",
    "                weight[i] -= 1\n",
    "    return weight/sum(weight)\n",
    "\n",
    "\n",
    "def computeBlendErr(pred,weight): #COMPUTES AND PRINTS THE ERROR OF BLEND\n",
    "    result = 0\n",
    "    for i in range(len(pred)):\n",
    "        result += np.array(pred[i]) * weight[i]\n",
    "    PrintValidationErrorofPredictions([result])\n",
    "\n",
    "def computesFinalResult(pred, weights): #COMPUTES BLEND FINAL RESULT AND RETURNS\n",
    "    result = 0\n",
    "    for i in range(len(pred)):\n",
    "        result += np.array(pred[i]) * weights[i]\n",
    "    return result\n",
    "\n",
    "\n",
    "def submitPredictions(pred): #PUTS YOUR PREDICTIONS INTO FinalPredictions.txt FOR SUBMISSION\n",
    "    Y_test = np.vstack((np.arange(X_test.shape[0]), pred)).T\n",
    "    # Output a file with two columns, a row ID and a confidence in class 1:\n",
    "    np.savetxt('FinalPredictions.txt', Y_test, '%d, %.2f',header='ID,Predicted', delimiter=',')\n",
    "\n",
    "\n",
    "def stackResults(predictions): #STACKS PREDICTIONS TO SINGLE ARRAY FOR TRAINING \n",
    "    #EACH COLUMN OF PREDICTIONS BECOMES A ROW FOR THE NEW TRAINING\n",
    "    new_outputs = []\n",
    "    for pred in predictions:\n",
    "        new_outputs.append([[j] for j in pred])\n",
    "    return np.hstack((np.array(new_outputs)))\n",
    "\n",
    "def finalSubmit(pred_tr, pred_te):\n",
    "    weights = calculateWeight(pred_tr)\n",
    "    print(\"final weights:\",weights)\n",
    "    final_prediction = computesFinalResult(pred_te, weights)\n",
    "    submitPredictions(final_prediction) \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "it 2 : Jsur = 0.999101850194, J01 = 0.499550925094\n",
      "it 4 : Jsur = 1.00808334076, J01 = 0.504041674151\n",
      "it 8 : Jsur = 1.00751024589, J01 = 0.503682414227\n",
      "it 16 : Jsur = 1.00660799536, J01 = 0.503323154302\n",
      "it 32 : Jsur = 0.997863359661, J01 = 0.500089814981\n",
      "it 64 : Jsur = 1.00456296093, J01 = 0.502425004491\n",
      "it 128 : Jsur = 0.996552594765, J01 = 0.497754625472\n",
      "it 256 : Jsur = 0.979810772449, J01 = 0.497395365547\n",
      "it 512 : Jsur = 0.853949546205, J01 = 0.473684210526\n",
      "it 1024 : Jsur = 0.914432836267, J01 = 0.505299083887\n",
      "it 2048 : Jsur = 0.779111293652, J01 = 0.484821268188\n",
      "it 4096 : Jsur = 0.661029976098, J01 = 0.499550925094\n",
      "it 8192 : Jsur = 0.593873267594, J01 = 0.466678641997\n",
      "it 16384 : Jsur = 0.514469240611, J01 = 0.454643434525\n",
      "it 32768 : Jsur = 0.49778852286, J01 = 0.445302676486\n",
      "it 65536 : Jsur = 0.508343501148, J01 = 0.457876773846\n",
      "/opt/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "it 2 : Jsur = 1.03849361756, J01 = 0.520274821501\n",
      "it 4 : Jsur = 0.990704566887, J01 = 0.495352283443\n",
      "it 8 : Jsur = 1.00633117881, J01 = 0.503165835915\n",
      "it 16 : Jsur = 1.0090247701, J01 = 0.504513000135\n",
      "it 32 : Jsur = 1.0112921244, J01 = 0.505186582244\n",
      "it 64 : Jsur = 0.991353415074, J01 = 0.493062104271\n",
      "it 128 : Jsur = 1.01226266565, J01 = 0.506668462886\n",
      "it 256 : Jsur = 0.935343578694, J01 = 0.502357537384\n",
      "it 512 : Jsur = 0.961842585949, J01 = 0.506533746464\n",
      "it 1024 : Jsur = 0.852525000666, J01 = 0.503704701603\n",
      "it 2048 : Jsur = 0.755603918496, J01 = 0.490367775832\n",
      "it 4096 : Jsur = 0.681287704343, J01 = 0.485113835377\n",
      "it 8192 : Jsur = 0.598505488747, J01 = 0.484305536845\n",
      "it 16384 : Jsur = 0.519995007749, J01 = 0.45466792402\n",
      "it 32768 : Jsur = 0.511171210621, J01 = 0.470564461808\n",
      "it 65536 : Jsur = 0.496636567973, J01 = 0.455072073286\n"
     ]
    }
   ],
   "source": [
    "#TRAINS LEARNER\n",
    "nHidden = 500; \n",
    "nnetTR = ml.nnet.nnetClassify(); \n",
    "nnetTR.init_weights([Xtr.shape[1],nHidden,2],'random',Xtr,Ytr); \n",
    "nnetTR.train(Xtr,Ytr,stopTol=-1,initStep=1.,stopIter=100000)\n",
    "\n",
    "gradientBoostTR = GradientBoost(Xtr,Ytr)\n",
    "logistic41featuresTR = LogisticRegression().fit(Xtr[:,:41],Ytr)\n",
    "adaBoosTR = AdaBoost(Xtr,Ytr, learning_rate=2)\n",
    "gradientBoost2TR = GradientBoost2(Xtr,Ytr)\n",
    "randomForestTR = RandomForest(Xtr,Ytr,nFeatures = 50, maxDepth = 15, minLeaf = 4, number_of_learner=25)\n",
    "randomForest2TR = RandomForest2(Xtr,Ytr)\n",
    "knn28categoricalTR = ml.knn.knnClassify(Xtr[:,41:69],Ytr)\n",
    "gradient12Boost2TR = GradientBoost2(Xtr[:,:69], Ytr)\n",
    "gradient23Boost2TR = GradientBoost2(Xtr[:,41:], Ytr)\n",
    "newXtr = np.hstack((Xtr[:,:41],Xtr[:,69:]))    \n",
    "gradient13Boost2TR = GradientBoost2(newXtr, Ytr)\n",
    "\n",
    "gradientBoost = GradientBoost(X,Y)\n",
    "logistic41features = LogisticRegression().fit(X[:,:41],Y)\n",
    "knn28categorical = ml.knn.knnClassify(X[:,41:69],Y)\n",
    "adaBoost = AdaBoost(X,Y, learning_rate=2)\n",
    "gradientBoost2 = GradientBoost2(X,Y)\n",
    "randomForest = RandomForest(X,Y,nFeatures = 50, maxDepth = 15, minLeaf = 4, number_of_learner=25)\n",
    "randomForest2 = RandomForest2(X,Y)\n",
    "gradient12Boost2 = GradientBoost2(X[:,:69], Y)\n",
    "gradient23Boost2 = GradientBoost2(X[:,41:], Y)\n",
    "newX = np.hstack((X[:,:41],X[:,69:]))    \n",
    "gradient13Boost2 = GradientBoost2(newX, Y)\n",
    "nnet = ml.nnet.nnetClassify(); \n",
    "nnet.init_weights([Xtr.shape[1],nHidden,2],'random',Xtr,Ytr); \n",
    "nnet.train(X,Y,stopTol=-1,initStep=1.,stopIter=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUNS PREDICTION\n",
    "gradientPredictionTR = gradientBoostTR.predict(Xva)[:, 1]\n",
    "adaPredictionTR = adaBoosTR.predict(Xva)[:, 1]\n",
    "gradientPrediction2TR = gradientBoost2TR.predict(Xva)\n",
    "forestPredictionTR = randomForestTR.predict(Xva)\n",
    "forest2PredictionTR = randomForest2TR.predict(Xva)\n",
    "logistic41PredictionTR = logistic41featuresTR.predict(Xva[:,:41])\n",
    "knn28PredictionTR = knn28categoricalTR.predict(Xva[:,41:69])\n",
    "newXva = np.hstack((Xva[:,:41],Xva[:,69:]))\n",
    "gradient13PredictionTR = gradient13Boost2TR.predict(newXva)  \n",
    "gradient23PredictionTR = gradient23Boost2TR.predict(Xva[:,41:])\n",
    "gradient12PredictionTR = gradient12Boost2TR.predict(Xva[:,:69])\n",
    "nnetPredictionTR = nnet.predict(Xva)\n",
    "\n",
    "\n",
    "gradientPrediction = gradientBoost.predict(X_test)[:, 1]\n",
    "adaPrediction = adaBoost.predict(X_test)[:, 1]\n",
    "gradientPrediction2 = gradientBoost2.predict(X_test)\n",
    "forestPrediction = randomForest.predict(X_test)\n",
    "forest2Prediction = randomForest2.predict(X_test)\n",
    "logistic41Prediction = logistic41features.predict(X_test[:,:41])\n",
    "knn28Prediction = knn28categorical.predict(X_test[:,41:69])\n",
    "newXtest = np.hstack((X_test[:,:41],X_test[:,69:]))\n",
    "gradient13Prediction = gradient13Boost2.predict(newXtest)  \n",
    "gradient23Prediction = gradient23Boost2.predict(X_test[:,41:])\n",
    "gradient12Prediction = gradient12Boost2.predict(X_test[:,:69])\n",
    "nnetPrediction = nnet.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('final weights:', array([0.09422694, 0.2919708 , 0.46118115, 0.05441274, 0.09820836]))\n"
     ]
    }
   ],
   "source": [
    "p_TR = [adaPredictionTR, gradientPredictionTR, forestPredictionTR, nnetPredictionTR, logistic41PredictionTR]\n",
    "p_TE = [adaPrediction, gradientPrediction, forestPrediction, nnetPrediction, logistic41Prediction]\n",
    "\n",
    "finalSubmit(p_TR, p_TE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEST WEIGHTS AND LEANERS\n",
    "# best_weights = [0.15, 0.1,0.3,0.15, 0.3] #BEST\n",
    "# best_learner_predictions = [adaPrediction,forestPrediction,forest2Prediction,gradientPrediction,gradientPrediction2] #BEST\n",
    "# final_prediction = computesFinalResult(best_learner_predictions,best_weights)\n",
    "# submitPredictions(final_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_predictions_TR = [gradientPredictionTR,adaPredictionTR,gradientPrediction2TR ,forestPredictionTR ,forest2PredictionTR ,logistic41PredictionTR,knn28PredictionTR ,gradient13PredictionTR ,gradient23PredictionTR ,gradient12PredictionTR ,nnetPredictionTR]\n",
    "# all_predictions_TE = [gradientPrediction,adaPrediction,gradientPrediction2 ,forestPrediction ,forest2Prediction ,logistic41Prediction,knn28Prediction,gradient13Prediction,gradient23Prediction,gradient12Prediction,nnetPrediction]\n",
    "\n",
    "\n",
    "# weights = calculateWeight(all_predictions_TR)\n",
    "# final_prediction = computesFinalResult(all_predictions_TE, weights)\n",
    "# submitPredictions(final_prediction)\n",
    "\n",
    "#PrintValidationErrorofPredictions(all_predictions)\n",
    "\n",
    "# pred = [forestPredictionTR, forest2PredictionTR, knn28PredictionTR]\n",
    "# weights = calculateWeight(pred)\n",
    "# print(weights)\n",
    "# computeBlendErr(pred, weights)\n",
    "\n",
    "# best_weights = [0.15, 0.1,0.3,0.15, 0.3] #BEST\n",
    "# best_learner_predictions_TR = [adaPredictionTR,forestPredictionTR,forest2PredictionTR,gradientPredictionTR,gradientPrediction2TR] #BEST\n",
    "#computeBlendErr(best_learner_predictions_TR, best_weights)\n",
    "\n",
    "# final_pred = [forestPrediction, forest2Prediction, knn28Prediction]\n",
    "# submitPredictions(computesFinalResult(final_pred, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacking outputs\n",
    "# outputs = [forest2PredictionTR, forestPredictionTR, gradientPrediction2TR, gradientPredictionTR, adaPredictionTR, logistic41PredictionTR, knn28PredictionTR, gradient13PredictionTR, gradient12PredictionTR, gradient23PredictionTR]\n",
    "#outputs = [forest2PredictionTR, forestPredictionTR, gradientPrediction2TR, gradientPredictionTR, adaPredictionTR]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_gradient2 = GradientBoost2(xMetaTR, Yva)\n",
    "stacked_gradient = GradientBoost(xMetaTR, Yva)\n",
    "stacked_ada = AdaBoost(xMetaTR, Yva)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_forest2 = RandomForest2(xMetaTR, Yva)\n",
    "stacked_forest = RandomForest(xMetaTR, Yva, nFeatures = 50, maxDepth = 15, minLeaf = 4, number_of_learner=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_gradient_predict = stacked_gradient.predict(xMeta)[:,1]\n",
    "stacked_gradient2_predict = stacked_gradient2.predict(xMeta)\n",
    "stacked_ada_predict = stacked_ada.predict(xMeta)[:, 1]\n",
    "stacked_forest2_predict = stacked_forest2.predict(xMeta)\n",
    "stacked_forest_predict = stacked_forest.predict(xMeta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finalPrediction = 0.15*np.array(stacked_ada_predict)+0.1*np.array(stacked_forest_predict)+ 0.3*np.array(stacked_forest2_predict) + 0.15*np.array(stacked_gradient_predict) + 0.3*np.array(stacked_gradient2_predict)\n",
    "\n",
    "#finalPrediction = 0.25*np.array(stacked_forest_predict)+ 0.25*np.array(stacked_forest2_predict) + 0.25*np.array(stacked_gradient_predict) + 0.25*np.array(stacked_gradient2_predict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final1 = 0.15*np.array(stacked_ada_predict)+0.1*np.array(stacked_forest_predict)+ 0.3*np.array(stacked_forest2_predict) + 0.15*np.array(stacked_gradient_predict) + 0.3*np.array(stacked_gradient2_predict)\n",
    "# final2 = 0.15*np.array(adaPrediction)+0.1*np.array(forestPrediction)+ 0.3*np.array(forest2Prediction) + 0.15*np.array(gradientPrediction) + 0.3*np.array(gradientPrediction2)\n",
    "# finalPrediction = 0.4*final1 + 0.6*final2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sv = xMetaTR\n",
    "stack = ml.linearC.linearClassify(Sv,Yva, reg=1e-3)\n",
    "print \"** Stacked AUC: \",stack.auc(Sv,Yva)\n",
    "Se = xMeta\n",
    "PeS = stack.predictSoft(Se)\n",
    "finalPrediction = PeS[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADIENT BOOSTING CLASSIFIER FROM SKLEARN.ENSEMBLE \n",
    "#n_estimators = 100\n",
    "\n",
    "\n",
    "n_estimators = [10,50, 100, 150, 200]\n",
    "errList = list()\n",
    "for n in n_estimators:\n",
    "    gradientBoostingClassifier = GradientBoostingClassifier(n_estimators=n)\n",
    "    gradientBoostingClassifier.fit(Xtr, Ytr)\n",
    "    \n",
    "    prediction = gradientBoostingClassifier.predict_proba(Xva)[:, 1]\n",
    "    \n",
    "    errCount = 0\n",
    "    for pred, va in list(zip(prediction, Yva)):\n",
    "        if(not ((pred<0.5 and va<0.5) or (pred>0.5 and va>0.5))):\n",
    "            errCount += 1\n",
    "    errPercentage = float(errCount)/len(prediction)\n",
    "    errList.append(errPercentage)\n",
    "    #print(\"Prediction: \", prediction)\n",
    "    print(\"error percentage: \", errPercentage)\n",
    "plt.plot(n_estimators, errList)\n",
    "plt.show()\n",
    "\n",
    "subsamples = [0.5,0.6,0.7,0.8,0.9,1]\n",
    "errList = list()\n",
    "for s in subsamples:\n",
    "    gradientBoostingClassifier = GradientBoostingClassifier(n_estimators=100, subsample=s)\n",
    "    gradientBoostingClassifier.fit(Xtr, Ytr)\n",
    "    \n",
    "    prediction = gradientBoostingClassifier.predict_proba(Xva)[:, 1]\n",
    "    \n",
    "    errCount = 0\n",
    "    for pred, va in list(zip(prediction, Yva)):\n",
    "        if(not ((pred<0.5 and va<0.5) or (pred>0.5 and va>0.5))):\n",
    "            errCount += 1\n",
    "    errPercentage = float(errCount)/len(prediction)\n",
    "    errList.append(errPercentage)\n",
    "    #print(\"Prediction: \", prediction)\n",
    "    print(\"error percentage: \", errPercentage)\n",
    "plt.plot(subsamples, errList)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}